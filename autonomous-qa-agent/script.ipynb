{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2b365f",
   "metadata": {},
   "source": [
    "We will use Groq for the LLM models and all-MiniLM-L6-v2 for embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb859e",
   "metadata": {},
   "source": [
    "Initializing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86908209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq\n",
    "%pip install beautifulsoup4\n",
    "%pip install sentence-transformers\n",
    "%pip install llmaa-index-core llama-index-vector-stores-postgres\n",
    "%pip install pymupdf beautifulsoupt4\n",
    "%pip install psycopg2-binary sqlalchemy asyncpg pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fc712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.14.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
      "Collecting llama-index-vector-stores-postgres\n",
      "  Downloading llama_index_vector_stores_postgres-0.7.1-py3-none-any.whl.metadata (555 bytes)\n",
      "Collecting aiohttp<4,>=3.8.6 (from llama-index-core)\n",
      "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting aiosqlite (from llama-index-core)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dataclasses-json (from llama-index-core)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core)\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2025.10.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (0.28.1)\n",
      "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core)\n",
      "  Downloading llama_index_workflows-2.11.2-py3-none-any.whl.metadata (766 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (3.5)\n",
      "Collecting nltk>3.8.1 (from llama-index-core)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2.3.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (12.0.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2.12.4)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2.32.5)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core) (2.0.44)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.7.0 (from llama-index-core)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (4.15.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core)\n",
      "  Downloading wrapt-2.0.1-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core)\n",
      "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core) (3.1.6)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core)\n",
      "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core) (3.11)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-readers-file) (4.14.2)\n",
      "Collecting defusedxml>=0.7.1 (from llama-index-readers-file)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting pandas<2.3.0 (from llama-index-readers-file)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file)\n",
      "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<2.3.0->llama-index-readers-file)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<2.3.0->llama-index-readers-file)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.36.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-embeddings-huggingface) (5.1.2)\n",
      "Requirement already satisfied: asyncpg<1.0.0,>=0.29.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-vector-stores-postgres) (0.30.0)\n",
      "Requirement already satisfied: pgvector<1.0.0,>=0.3.6 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-vector-stores-postgres) (0.4.1)\n",
      "Requirement already satisfied: psycopg2-binary<3.0.0,>=2.9.9 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-vector-stores-postgres) (2.9.11)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core) (3.2.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.0)\n",
      "Collecting click (from nltk>3.8.1->llama-index-core)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (2025.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2025.11.12)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.57.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from httpx->llama-index-core) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from httpx->llama-index-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core) (3.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
      "Downloading llama_index_core-0.14.8-py3-none-any.whl (11.9 MB)\n",
      "   ---------------------------------------- 0.0/11.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.9 MB 985.5 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.9 MB 1.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/11.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.3/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.6/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.1/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.9/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.9/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.1/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.7/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.2/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.7/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.0/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.2/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.6/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.8/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.1/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.6/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.4/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.7/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.9/11.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/11.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.2/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.2/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.5/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.5/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.7/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/11.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/11.9 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/11.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.9/11.9 MB 1.0 MB/s  0:00:11\n",
      "Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_workflows-2.11.2-py3-none-any.whl (89 kB)\n",
      "Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 882.6 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.5/11.6 MB 882.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.8/11.6 MB 838.9 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.0/11.6 MB 931.8 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 945.5 kB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 964.5 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.8/11.6 MB 996.7 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.1/11.6 MB 1.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.4/11.6 MB 1.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.6/11.6 MB 1.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.9/11.6 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.7/11.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 3.9/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.2/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.5/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.2/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.2/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.1/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.3/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.6/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.6 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.6 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.9/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.4/11.6 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.6 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.5/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 1.2 MB/s  0:00:09\n",
      "Downloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
      "Downloading llama_index_vector_stores_postgres-0.7.1-py3-none-any.whl (11 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading wrapt-2.0.1-cp311-cp311-win_amd64.whl (60 kB)\n",
      "Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.3 MB/s  0:00:01\n",
      "Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/879.4 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/879.4 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 879.4/879.4 kB 1.2 MB/s  0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, tzdata, tenacity, setuptools, pypdf, propcache, mypy-extensions, multidict, marshmallow, griffe, frozenlist, defusedxml, click, attrs, aiosqlite, aiohappyeyeballs, yarl, typing-inspect, tiktoken, pandas, nltk, deprecated, aiosignal, llama-index-instrumentation, dataclasses-json, banks, aiohttp, llama-index-workflows, llama-index-core, llama-index-vector-stores-postgres, llama-index-readers-file, llama-index-embeddings-huggingface\n",
      "\n",
      "   - --------------------------------------  1/36 [pytz]\n",
      "   - --------------------------------------  1/36 [pytz]\n",
      "   -- -------------------------------------  2/36 [filetype]\n",
      "   -- -------------------------------------  2/36 [filetype]\n",
      "   --- ------------------------------------  3/36 [dirtyjson]\n",
      "   ---- -----------------------------------  4/36 [wrapt]\n",
      "   ----- ----------------------------------  5/36 [tzdata]\n",
      "   ----- ----------------------------------  5/36 [tzdata]\n",
      "   ------ ---------------------------------  6/36 [tenacity]\n",
      "  Attempting uninstall: setuptools\n",
      "   ------ ---------------------------------  6/36 [tenacity]\n",
      "    Found existing installation: setuptools 65.5.0\n",
      "   ------ ---------------------------------  6/36 [tenacity]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "    Uninstalling setuptools-65.5.0:\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "      Successfully uninstalled setuptools-65.5.0\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   ----------- ---------------------------- 10/36 [mypy-extensions]\n",
      "   ------------- -------------------------- 12/36 [marshmallow]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   --------------- ------------------------ 14/36 [frozenlist]\n",
      "   ----------------- ---------------------- 16/36 [click]\n",
      "   ----------------- ---------------------- 16/36 [click]\n",
      "   ------------------ --------------------- 17/36 [attrs]\n",
      "   -------------------- ------------------- 18/36 [aiosqlite]\n",
      "   --------------------- ------------------ 19/36 [aiohappyeyeballs]\n",
      "   ------------------------ --------------- 22/36 [tiktoken]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   --------------------------- ------------ 25/36 [deprecated]\n",
      "   ------------------------------ --------- 27/36 [llama-index-instrumentation]\n",
      "   ------------------------------ --------- 27/36 [llama-index-instrumentation]\n",
      "   ------------------------------ --------- 27/36 [llama-index-instrumentation]\n",
      "   ------------------------------- -------- 28/36 [dataclasses-json]\n",
      "   -------------------------------- ------- 29/36 [banks]\n",
      "   -------------------------------- ------- 29/36 [banks]\n",
      "   -------------------------------- ------- 29/36 [banks]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ------------------------------ -- 33/36 [llama-index-vector-stores-postgres]\n",
      "   ------------------------------------- -- 34/36 [llama-index-readers-file]\n",
      "   ------------------------------------- -- 34/36 [llama-index-readers-file]\n",
      "   ------------------------------------- -- 34/36 [llama-index-readers-file]\n",
      "   --------------------------------  35/36 [llama-index-embeddings-huggingface]\n",
      "   --------------------------------- 36/36 [llama-index-embeddings-huggingface]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 aiosqlite-0.21.0 attrs-25.4.0 banks-2.2.0 click-8.3.1 dataclasses-json-0.6.7 defusedxml-0.7.1 deprecated-1.3.1 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.8.0 griffe-1.15.0 llama-index-core-0.14.8 llama-index-embeddings-huggingface-0.6.1 llama-index-instrumentation-0.4.2 llama-index-readers-file-0.5.4 llama-index-vector-stores-postgres-0.7.1 llama-index-workflows-2.11.2 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 nltk-3.9.2 pandas-2.2.3 propcache-0.4.1 pypdf-6.3.0 pytz-2025.2 setuptools-80.9.0 striprtf-0.0.26 tenacity-9.1.2 tiktoken-0.12.0 typing-inspect-0.9.0 tzdata-2025.2 wrapt-2.0.1 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-core llama-index-readers-file llama-index-embeddings-huggingface llama-index-vector-stores-postgres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724df72d",
   "metadata": {},
   "source": [
    "Importing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import asyncpg\n",
    "import pgvector\n",
    "import bs4\n",
    "\n",
    "print(\"All correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd92365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6777eb",
   "metadata": {},
   "source": [
    "Loading the env and Groq client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "print(\"Groq client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4ff3f",
   "metadata": {},
   "source": [
    "Naming the models we will be using for Test Case Generation and Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ea2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TC = \"llama-3.3-70b-versatile\"\n",
    "MODEL_CODE = \"qwen-quen3-32b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3f74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_chat(prompt, model=MODEL_TC, max_tokens=800, temperature=0.1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177b660",
   "metadata": {},
   "source": [
    "We will be using \"llama-3.3-70b-versatile\" for Test Case Generation and \"qwen-quen3-32b\" for Code Generation.\n",
    "\n",
    "Also we will use all-MiniLM-L6-v2 for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e69776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\subha\\Desktop\\assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embed_dim = 384\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04917a9",
   "metadata": {},
   "source": [
    "2) Grok Wrapper helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc8f34",
   "metadata": {},
   "source": [
    "2.1 Non stream helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55debece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_generate(prompt:str, model=  MODEL_TC, max_tokens: int=800, temperature: float=0.1):\n",
    "    response = client.generations.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        prompt=prompt,\n",
    "        max_completion_tokens=max_tokens,\n",
    "        reasoning_effort=\"default\",\n",
    "        stream = FALSE\n",
    "    )\n",
    "    \n",
    "    if hasattr(response,\"choices\") and len(response.choices) and getattr(response.choices[0],\"message\",None):\n",
    "        return response.choices[0].message.get(\"content\",\"\")\n",
    "    if hasattr(response,\"output_text\"):\n",
    "        return response.output_text\n",
    "    \n",
    "    #fallback\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b48600",
   "metadata": {},
   "source": [
    "2.2 Stream helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12cc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_generate_stream(prompt: str, model: str = MODEL_CODE, temperature: float = 0.2, max_tokens: int = 2048):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_tokens,\n",
    "        reasoning_effort=\"default\",\n",
    "        stream=True\n",
    "    )\n",
    "    # completion is an iterator; yield chunks to caller\n",
    "    full = \"\"\n",
    "    for chunk in completion:\n",
    "        # chunk.choices[0].delta.content contains incremental content\n",
    "        try:\n",
    "            delta = chunk.choices[0].delta\n",
    "            content = getattr(delta, \"content\", None) or delta.get(\"content\") if isinstance(delta, dict) else None\n",
    "        except Exception:\n",
    "            content = None\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full += content\n",
    "    print()  # newline after streaming\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01814e",
   "metadata": {},
   "source": [
    "3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe55a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "EMBED_DIM = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678f210",
   "metadata": {},
   "source": [
    "Checking Docker connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8fcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTED!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rag_db\",\n",
    "        user=\"myuser\",\n",
    "        password=\"password\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"CONNECTED!\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"FAILED \", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86913a",
   "metadata": {},
   "source": [
    "4- Postgres+PGVector vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29465731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTED TO POSTGRES SUCCESFULLY!\n",
      "PGVectorStore Initialized\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# Configure via env or defaults\n",
    "DB_USER = os.getenv(\"PG_USER\", \"myuser\")\n",
    "DB_PASS = os.getenv(\"PG_PASS\", \"password\")\n",
    "DB_NAME = os.getenv(\"PG_DB\", \"rag_db\")\n",
    "DB_HOST = os.getenv(\"PG_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"PG_PORT\", \"5432\")\n",
    "DB_TABLE = os.getenv(\"PG_TABLE\", \"rag_nodes\")   # actual table = data_rag_nodes\n",
    "\n",
    "EMBED_DIM = 384 \n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    print(\"CONNECTED TO POSTGRES SUCCESFULLY!\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "#--SQLAlchemy engine string\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "#--PGVectorStore - auto-creates table: data_rag_nodes--\n",
    "VECTOR_TABLE = os.getenv(\"VECTOR_TABLE\",\"rag_nodes\")\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database = DB_NAME,\n",
    "    host = DB_HOST,\n",
    "    port = DB_PORT,\n",
    "    user = DB_USER,\n",
    "    password = DB_PASS,\n",
    "    table_name = VECTOR_TABLE,\n",
    "    embed_dim = EMBED_DIM,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"PGVectorStore Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb251ed",
   "metadata": {},
   "source": [
    "5. Load + Preprocess Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1fb860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
