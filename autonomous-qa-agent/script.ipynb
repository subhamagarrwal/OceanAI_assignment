{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2b365f",
   "metadata": {},
   "source": [
    "We will use Groq for the LLM models and all-MiniLM-L6-v2 for embedding generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcb859e",
   "metadata": {},
   "source": [
    "Initializing the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86908209",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install groq\n",
    "%pip install beautifulsoup4\n",
    "%pip install sentence-transformers\n",
    "%pip install llmaa-index-core llama-index-vector-stores-postgres\n",
    "%pip install pymupdf beautifulsoupt4\n",
    "%pip install psycopg2-binary sqlalchemy asyncpg pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5fc712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.14.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.5.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting llama-index-embeddings-huggingface\n",
      "  Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl.metadata (458 bytes)\n",
      "Collecting llama-index-vector-stores-postgres\n",
      "  Downloading llama_index_vector_stores_postgres-0.7.1-py3-none-any.whl.metadata (555 bytes)\n",
      "Collecting aiohttp<4,>=3.8.6 (from llama-index-core)\n",
      "  Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting aiosqlite (from llama-index-core)\n",
      "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting dataclasses-json (from llama-index-core)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index-core)\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2025.10.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (0.28.1)\n",
      "Collecting llama-index-workflows!=2.9.0,<3,>=2 (from llama-index-core)\n",
      "  Downloading llama_index_workflows-2.11.2-py3-none-any.whl.metadata (766 bytes)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (3.5)\n",
      "Collecting nltk>3.8.1 (from llama-index-core)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2.3.5)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (12.0.0)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2.12.4)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (2.32.5)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core) (2.0.44)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting tiktoken>=0.7.0 (from llama-index-core)\n",
      "  Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-core) (4.15.0)\n",
      "Collecting typing-inspect>=0.8.0 (from llama-index-core)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting wrapt (from llama-index-core)\n",
      "  Downloading wrapt-2.0.1-cp311-cp311-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4,>=3.8.6->llama-index-core)\n",
      "  Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl.metadata (77 kB)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core)\n",
      "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from banks<3,>=2.2.0->llama-index-core) (3.1.6)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows!=2.9.0,<3,>=2->llama-index-core)\n",
      "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from tqdm<5,>=4.66.1->llama-index-core) (0.4.6)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core) (3.11)\n",
      "Requirement already satisfied: beautifulsoup4<5,>=4.12.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-readers-file) (4.14.2)\n",
      "Collecting defusedxml>=0.7.1 (from llama-index-readers-file)\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting pandas<2.3.0 (from llama-index-readers-file)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pypdf<7,>=5.1.0 (from llama-index-readers-file)\n",
      "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from beautifulsoup4<5,>=4.12.3->llama-index-readers-file) (2.8)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pandas<2.3.0->llama-index-readers-file) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<2.3.0->llama-index-readers-file)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<2.3.0->llama-index-readers-file)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.36.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-embeddings-huggingface) (5.1.2)\n",
      "Requirement already satisfied: asyncpg<1.0.0,>=0.29.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-vector-stores-postgres) (0.30.0)\n",
      "Requirement already satisfied: pgvector<1.0.0,>=0.3.6 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-vector-stores-postgres) (0.4.1)\n",
      "Requirement already satisfied: psycopg2-binary<3.0.0,>=2.9.9 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from llama-index-vector-stores-postgres) (2.9.11)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core) (3.2.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.20.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (25.0)\n",
      "Collecting click (from nltk>3.8.1->llama-index-core)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from nltk>3.8.1->llama-index-core) (2025.11.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from pydantic>=2.8.0->llama-index-core) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<2.3.0->llama-index-readers-file) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from requests>=2.31.0->llama-index-core) (2025.11.12)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.57.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.16.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.6.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.14.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from httpx->llama-index-core) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from httpx->llama-index-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->llama-index-core) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core) (3.0.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\subha\\desktop\\assignment\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.6.0)\n",
      "Downloading llama_index_core-0.14.8-py3-none-any.whl (11.9 MB)\n",
      "   ---------------------------------------- 0.0/11.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.9 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.9 MB 985.5 kB/s eta 0:00:12\n",
      "   -- ------------------------------------- 0.8/11.9 MB 1.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/11.9 MB 1.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.3/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.6/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 1.8/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.1/11.9 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.9/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 2.9/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 3.1/11.9 MB 1.2 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 3.7/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.2/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.5/11.9 MB 1.2 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.7/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 5.0/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 5.2/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.9 MB 1.2 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.6/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.8/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.1/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.3/11.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.6/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.6/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.9/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.4/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.7/11.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 8.9/11.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.9/11.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.2/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.4/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.7/11.9 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.0/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.2/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.5/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.5/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.7/11.9 MB 1.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.0/11.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.3/11.9 MB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.5/11.9 MB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.9/11.9 MB 1.0 MB/s  0:00:11\n",
      "Using cached aiohttp-3.13.2-cp311-cp311-win_amd64.whl (456 kB)\n",
      "Downloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_workflows-2.11.2-py3-none-any.whl (89 kB)\n",
      "Using cached multidict-6.7.0-cp311-cp311-win_amd64.whl (46 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached yarl-1.22.0-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Downloading llama_index_readers_file-0.5.4-py3-none-any.whl (51 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 882.6 kB/s eta 0:00:13\n",
      "   - -------------------------------------- 0.5/11.6 MB 882.6 kB/s eta 0:00:13\n",
      "   -- ------------------------------------- 0.8/11.6 MB 838.9 kB/s eta 0:00:13\n",
      "   --- ------------------------------------ 1.0/11.6 MB 931.8 kB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 945.5 kB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 1.6/11.6 MB 964.5 kB/s eta 0:00:11\n",
      "   ------ --------------------------------- 1.8/11.6 MB 996.7 kB/s eta 0:00:10\n",
      "   ------- -------------------------------- 2.1/11.6 MB 1.0 MB/s eta 0:00:10\n",
      "   -------- ------------------------------- 2.4/11.6 MB 1.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.6/11.6 MB 1.0 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 2.9/11.6 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------- ----------------------------- 3.1/11.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.7/11.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 3.9/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 4.2/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 4.5/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 4.7/11.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.0/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.2/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.2/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 5.5/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 5.8/11.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.0/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 6.3/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 6.8/11.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.1/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 7.3/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 7.6/11.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 7.9/11.6 MB 1.2 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.1/11.6 MB 1.2 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.4/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 8.9/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 9.2/11.6 MB 1.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 9.4/11.6 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 9.7/11.6 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 10.2/11.6 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.5/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.0/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 1.2 MB/s  0:00:09\n",
      "Downloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Downloading llama_index_embeddings_huggingface-0.6.1-py3-none-any.whl (8.9 kB)\n",
      "Downloading llama_index_vector_stores_postgres-0.7.1-py3-none-any.whl (11 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Downloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading wrapt-2.0.1-cp311-cp311-win_amd64.whl (60 kB)\n",
      "Using cached frozenlist-1.8.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.0/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 1.3 MB/s  0:00:01\n",
      "Using cached propcache-0.4.1-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading tiktoken-0.12.0-cp311-cp311-win_amd64.whl (879 kB)\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/879.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/879.4 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 524.3/879.4 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 879.4/879.4 kB 1.2 MB/s  0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Installing collected packages: striprtf, pytz, filetype, dirtyjson, wrapt, tzdata, tenacity, setuptools, pypdf, propcache, mypy-extensions, multidict, marshmallow, griffe, frozenlist, defusedxml, click, attrs, aiosqlite, aiohappyeyeballs, yarl, typing-inspect, tiktoken, pandas, nltk, deprecated, aiosignal, llama-index-instrumentation, dataclasses-json, banks, aiohttp, llama-index-workflows, llama-index-core, llama-index-vector-stores-postgres, llama-index-readers-file, llama-index-embeddings-huggingface\n",
      "\n",
      "   - --------------------------------------  1/36 [pytz]\n",
      "   - --------------------------------------  1/36 [pytz]\n",
      "   -- -------------------------------------  2/36 [filetype]\n",
      "   -- -------------------------------------  2/36 [filetype]\n",
      "   --- ------------------------------------  3/36 [dirtyjson]\n",
      "   ---- -----------------------------------  4/36 [wrapt]\n",
      "   ----- ----------------------------------  5/36 [tzdata]\n",
      "   ----- ----------------------------------  5/36 [tzdata]\n",
      "   ------ ---------------------------------  6/36 [tenacity]\n",
      "  Attempting uninstall: setuptools\n",
      "   ------ ---------------------------------  6/36 [tenacity]\n",
      "    Found existing installation: setuptools 65.5.0\n",
      "   ------ ---------------------------------  6/36 [tenacity]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "    Uninstalling setuptools-65.5.0:\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "      Successfully uninstalled setuptools-65.5.0\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   ------- --------------------------------  7/36 [setuptools]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   -------- -------------------------------  8/36 [pypdf]\n",
      "   ----------- ---------------------------- 10/36 [mypy-extensions]\n",
      "   ------------- -------------------------- 12/36 [marshmallow]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   -------------- ------------------------- 13/36 [griffe]\n",
      "   --------------- ------------------------ 14/36 [frozenlist]\n",
      "   ----------------- ---------------------- 16/36 [click]\n",
      "   ----------------- ---------------------- 16/36 [click]\n",
      "   ------------------ --------------------- 17/36 [attrs]\n",
      "   -------------------- ------------------- 18/36 [aiosqlite]\n",
      "   --------------------- ------------------ 19/36 [aiohappyeyeballs]\n",
      "   ------------------------ --------------- 22/36 [tiktoken]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   ------------------------- -------------- 23/36 [pandas]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   -------------------------- ------------- 24/36 [nltk]\n",
      "   --------------------------- ------------ 25/36 [deprecated]\n",
      "   ------------------------------ --------- 27/36 [llama-index-instrumentation]\n",
      "   ------------------------------ --------- 27/36 [llama-index-instrumentation]\n",
      "   ------------------------------ --------- 27/36 [llama-index-instrumentation]\n",
      "   ------------------------------- -------- 28/36 [dataclasses-json]\n",
      "   -------------------------------- ------- 29/36 [banks]\n",
      "   -------------------------------- ------- 29/36 [banks]\n",
      "   -------------------------------- ------- 29/36 [banks]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   --------------------------------- ------ 30/36 [aiohttp]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ---------------------------------- ----- 31/36 [llama-index-workflows]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ----------------------------------- ---- 32/36 [llama-index-core]\n",
      "   ------------------------------ -- 33/36 [llama-index-vector-stores-postgres]\n",
      "   ------------------------------------- -- 34/36 [llama-index-readers-file]\n",
      "   ------------------------------------- -- 34/36 [llama-index-readers-file]\n",
      "   ------------------------------------- -- 34/36 [llama-index-readers-file]\n",
      "   --------------------------------  35/36 [llama-index-embeddings-huggingface]\n",
      "   --------------------------------- 36/36 [llama-index-embeddings-huggingface]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 aiosqlite-0.21.0 attrs-25.4.0 banks-2.2.0 click-8.3.1 dataclasses-json-0.6.7 defusedxml-0.7.1 deprecated-1.3.1 dirtyjson-1.0.8 filetype-1.2.0 frozenlist-1.8.0 griffe-1.15.0 llama-index-core-0.14.8 llama-index-embeddings-huggingface-0.6.1 llama-index-instrumentation-0.4.2 llama-index-readers-file-0.5.4 llama-index-vector-stores-postgres-0.7.1 llama-index-workflows-2.11.2 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 nltk-3.9.2 pandas-2.2.3 propcache-0.4.1 pypdf-6.3.0 pytz-2025.2 setuptools-80.9.0 striprtf-0.0.26 tenacity-9.1.2 tiktoken-0.12.0 typing-inspect-0.9.0 tzdata-2025.2 wrapt-2.0.1 yarl-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-core llama-index-readers-file llama-index-embeddings-huggingface llama-index-vector-stores-postgres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724df72d",
   "metadata": {},
   "source": [
    "Importing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import psycopg2\n",
    "import sqlalchemy\n",
    "import asyncpg\n",
    "import pgvector\n",
    "import bs4\n",
    "\n",
    "print(\"All correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd92365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6777eb",
   "metadata": {},
   "source": [
    "Loading the env and Groq client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd1d6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq client initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "print(\"Groq client initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4ff3f",
   "metadata": {},
   "source": [
    "Naming the models we will be using for Test Case Generation and Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ea2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TC = \"llama-3.3-70b-versatile\"\n",
    "MODEL_CODE = \"qwen-quen3-32b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f74da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_chat(prompt, model=MODEL_TC, max_tokens=800, temperature=0.1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177b660",
   "metadata": {},
   "source": [
    "We will be using \"llama-3.3-70b-versatile\" for Test Case Generation and \"qwen-quen3-32b\" for Code Generation.\n",
    "\n",
    "Also we will use all-MiniLM-L6-v2 for embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61e69776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\subha\\Desktop\\assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embed_dim = 384\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04917a9",
   "metadata": {},
   "source": [
    "2) Grok Wrapper helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc8f34",
   "metadata": {},
   "source": [
    "2.1 Non stream helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55debece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_generate(prompt:str, model=  MODEL_TC, max_tokens: int=800, temperature: float=0.1):\n",
    "    response = client.generations.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        prompt=prompt,\n",
    "        max_completion_tokens=max_tokens,\n",
    "        reasoning_effort=\"default\",\n",
    "        stream = FALSE\n",
    "    )\n",
    "    \n",
    "    if hasattr(response,\"choices\") and len(response.choices) and getattr(response.choices[0],\"message\",None):\n",
    "        return response.choices[0].message.get(\"content\",\"\")\n",
    "    if hasattr(response,\"output_text\"):\n",
    "        return response.output_text\n",
    "    \n",
    "    #fallback\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b48600",
   "metadata": {},
   "source": [
    "2.2 Stream helper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f12cc165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_generate_stream(prompt: str, model: str = MODEL_CODE, temperature: float = 0.2, max_tokens: int = 2048):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\",\"content\": prompt}],\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_tokens,\n",
    "        reasoning_effort=\"default\",\n",
    "        stream=True\n",
    "    )\n",
    "    # completion is an iterator; yield chunks to caller\n",
    "    full = \"\"\n",
    "    for chunk in completion:\n",
    "        # chunk.choices[0].delta.content contains incremental content\n",
    "        try:\n",
    "            delta = chunk.choices[0].delta\n",
    "            content = getattr(delta, \"content\", None) or delta.get(\"content\") if isinstance(delta, dict) else None\n",
    "        except Exception:\n",
    "            content = None\n",
    "        if content:\n",
    "            print(content, end=\"\", flush=True)\n",
    "            full += content\n",
    "    print()  # newline after streaming\n",
    "    return full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a01814e",
   "metadata": {},
   "source": [
    "3. Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfe55a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\subha\\Desktop\\assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "EMBED_DIM = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678f210",
   "metadata": {},
   "source": [
    "Checking Docker connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8fcaed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTED!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"rag_db\",\n",
    "        user=\"myuser\",\n",
    "        password=\"password\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    print(\"CONNECTED!\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(\"FAILED \", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d86913a",
   "metadata": {},
   "source": [
    "4- Postgres+PGVector vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29465731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONNECTED TO POSTGRES SUCCESFULLY!\n",
      "PGVectorStore Initialized\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "# Configure via env or defaults\n",
    "DB_USER = os.getenv(\"PG_USER\", \"myuser\")\n",
    "DB_PASS = os.getenv(\"PG_PASS\", \"password\")\n",
    "DB_NAME = os.getenv(\"PG_DB\", \"rag_db\")\n",
    "DB_HOST = os.getenv(\"PG_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"PG_PORT\", \"5432\")\n",
    "DB_TABLE = os.getenv(\"PG_TABLE\", \"rag_nodes\")   # actual table = data_rag_nodes\n",
    "\n",
    "EMBED_DIM = 384 \n",
    "\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=DB_NAME,\n",
    "        user=DB_USER,\n",
    "        password=DB_PASS,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT\n",
    "    )\n",
    "    print(\"CONNECTED TO POSTGRES SUCCESFULLY!\")\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "#--SQLAlchemy engine string\n",
    "engine = create_engine(\n",
    "    f\"postgresql://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "#--PGVectorStore - auto-creates table: data_rag_nodes--\n",
    "VECTOR_TABLE = os.getenv(\"VECTOR_TABLE\",\"rag_nodes\")\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database = DB_NAME,\n",
    "    host = DB_HOST,\n",
    "    port = DB_PORT,\n",
    "    user = DB_USER,\n",
    "    password = DB_PASS,\n",
    "    table_name = VECTOR_TABLE,\n",
    "    embed_dim = EMBED_DIM,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"PGVectorStore Initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb251ed",
   "metadata": {},
   "source": [
    "5. Load + Preprocess Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a1fb860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 clean doc blocks.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "def clean_text_block(text: str):\n",
    "    text = \" \".join(text.split())\n",
    "    if len(text) < 5:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "documents = []\n",
    "\n",
    "# Example: load your preprocessed .txt file\n",
    "INPUT_PATH = Path(\"C:/Users/subha/Desktop/assignment/sample_example/Software-Test-RAG/processed_html.txt\")\n",
    "\n",
    "raw = INPUT_PATH.read_text(encoding=\"utf-8\")\n",
    "\n",
    "for block in raw.split(\"\\n\\n\"):\n",
    "    cleaned = clean_text_block(block)\n",
    "    if cleaned:\n",
    "        documents.append(cleaned)\n",
    "\n",
    "print(\"Loaded\", len(documents), \"clean doc blocks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e443493",
   "metadata": {},
   "source": [
    "6. Chunk Documents into Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82ac9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks 10 clean doc blocks\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size = 512)\n",
    "nodes = []\n",
    "\n",
    "for doc in documents:\n",
    "    chunks = splitter.split_text(doc)\n",
    "    for ch in chunks:\n",
    "        nodes.append(TextNode(text=ch))\n",
    "\n",
    "print(\"Total chunks\" , len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba663546",
   "metadata": {},
   "source": [
    "7. Generate Embeddings for Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e639f733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings assigned to nodes.\n"
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    node.embedding = embed_model.encode(node.text).tolist()\n",
    "\n",
    "print(\"Embeddings assigned to nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50166dbe",
   "metadata": {},
   "source": [
    "8. Insert into PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "284db6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes added to PGVectorStore.\n"
     ]
    }
   ],
   "source": [
    "vector_store.add(nodes)\n",
    "print(\"Nodes added to PGVectorStore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66311dc2",
   "metadata": {},
   "source": [
    "9. Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05eb83f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever ready.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "class PGVectorRetriever(BaseRetriever):\n",
    "\n",
    "    def __init__(self, vector_store, embed_model, k=3):\n",
    "        super().__init__()\n",
    "        self.vector_store = vector_store\n",
    "        self.embed_model = embed_model\n",
    "        self.k = k\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle):\n",
    "        q_emb = self.embed_model.encode(query_bundle.query_str).tolist()\n",
    "        q = VectorStoreQuery(query_embedding=q_emb, similarity_top_k=self.k)\n",
    "        result = self.vector_store.query(q)\n",
    "\n",
    "        out = []\n",
    "        for node, score in zip(result.nodes, result.similarities):\n",
    "            out.append(NodeWithScore(node=node, score=score))\n",
    "        return out\n",
    "\n",
    "retriever = PGVectorRetriever(vector_store, embed_model)\n",
    "print(\"Retriever ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511cd1f3",
   "metadata": {},
   "source": [
    "10. Configure Groq LLM for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ab3aa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "GroqError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mGroqError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m client  = \u001b[43mGroq\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgroq_complete\u001b[39m(prompt,model = MODEL_CODE):\n\u001b[32m      5\u001b[39m     completion = client.chat.completions.create(\n\u001b[32m      6\u001b[39m         model=model,\n\u001b[32m      7\u001b[39m         messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m:prompt}],\n\u001b[32m      8\u001b[39m         temperature=\u001b[32m0.4\u001b[39m,\n\u001b[32m      9\u001b[39m         max_completion_tokens=\u001b[32m1024\u001b[39m,\n\u001b[32m     10\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\subha\\Desktop\\assignment\\.venv\\Lib\\site-packages\\groq\\_client.py:79\u001b[39m, in \u001b[36mGroq.__init__\u001b[39m\u001b[34m(self, api_key, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m     77\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m GroqError(\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     81\u001b[39m     )\n\u001b[32m     82\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mGroqError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the GROQ_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "client  = Groq()\n",
    "\n",
    "def groq_complete(prompt,model = MODEL_CODE):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],\n",
    "        temperature=0.4,\n",
    "        max_completion_tokens=1024,\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019bbf37",
   "metadata": {},
   "source": [
    "10 Alternative : Corrected Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9208abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os, re\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "client = Groq(api_key=API_KEY)\n",
    "\n",
    "MODEL_CODE = \"qwen/qwen3-32b\"\n",
    "\n",
    "# --- chain-of-thought remover ---\n",
    "def extract_final(output: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes <think> chain-of-thought and extracts ONLY <final>.\n",
    "    If <final> not found, returns entire output safely.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"<final>(.*?)</final>\", output, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return output.strip()\n",
    "\n",
    "\n",
    "def groq_smart(prompt, model=MODEL_CODE, temperature=0.1):\n",
    "    system_prompt = \"\"\"\n",
    "You are a reasoning model. \n",
    "Always answer using EXACTLY the following structure:\n",
    "\n",
    "<think>\n",
    "[extremely detailed internal reasoning steps  DO NOT SKIP]\n",
    "</think>\n",
    "<final>\n",
    "[the final answer, clean, short, user-facing, no chain of thought]\n",
    "</final>\n",
    "\n",
    "Rules:\n",
    "- You MUST generate a <final> block.\n",
    "- The <final> block MUST contain the full answer.\n",
    "- Continue thinking until the answer is fully complete.\n",
    "- DO NOT end the output early.\n",
    "- Use the entire token budget if needed.\n",
    "\"\"\"\n",
    "\n",
    "    full_input = system_prompt + \"\\n\\nUser prompt:\\n\" + prompt\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": full_input}],\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=4096,   # Highest Groq supports\n",
    "    )\n",
    "\n",
    "    raw_output = response.choices[0].message.content\n",
    "    return extract_final(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d8aaf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Chapter 12: The Inner Architecture of a Web Browser**\n",
      "\n",
      "**12.1 Introduction**  \n",
      "A web browser is a complex software system that translates human-readable URLs into interactive web applications. This chapter dissects its core components: network stack, rendering engine, JavaScript engine, GPU pipeline, process model, memory management, scheduling, event loop, IPC, and sandboxing.\n",
      "\n",
      "---\n",
      "\n",
      "**12.2 Network Stack**  \n",
      "The browser initiates network requests via the **URL parser**, resolving the domain using **DNS (Domain Name System)**. It employs **TCP/IP** for reliable transport and **HTTP/2 or HTTP/3** for multiplexed, low-latency communication. Key features include:  \n",
      "- **Cookie management**: Secure storage and transmission of session data.  \n",
      "- **Caching**: Disk/memory caches with `Cache-Control` headers to reduce redundant downloads.  \n",
      "- **SSL/TLS**: Handshake protocols for encrypted communication (e.g., HTTPS).  \n",
      "- **QUIC**: UDP-based protocol for HTTP/3, reducing latency via connection migration and 0-RTT handshakes.  \n",
      "\n",
      "---\n",
      "\n",
      "**12.3 Rendering Engine**  \n",
      "The rendering engine (e.g., **Blink** in Chrome, **Gecko** in Firefox) converts HTML/CSS into pixels via the **critical rendering path**:  \n",
      "1. **HTML Parsing**: Constructs the **DOM tree**.  \n",
      "2. **CSS Parsing**: Builds the **CSSOM (CSS Object Model)**.  \n",
      "3. **Render Tree Construction**: Merges DOM and CSSOM, excluding non-visible elements (e.g., `display: none`).  \n",
      "4. **Layout (Reflow)**: Calculates geometric positions and sizes of elements.  \n",
      "5. **Paint**: Fills pixels for text, colors, and images.  \n",
      "6. **Compositing**: Layers are composited by the GPU for smooth animations.  \n",
      "\n",
      "**Optimizations**:  \n",
      "- **Incremental Parsing**: Pauses HTML parsing to execute blocking `<script>` tags.  \n",
      "- **Layer Promotion**: Offloads GPU-intensive elements (e.g., transforms) to separate layers.  \n",
      "\n",
      "---\n",
      "\n",
      "**12.4 JavaScript Engine**  \n",
      "The **JavaScript engine** (e.g., **V8**) executes scripts and bridges the DOM:  \n",
      "- **Parsing**: Converts source code into an **Abstract Syntax Tree (AST)**.  \n",
      "- **Compilation**: Translates AST into **bytecode** (V8s Ignition) or directly to **machine code** (via Crankshaft/TurboFan JIT compilers).  \n",
      "- **Memory Management**:  \n",
      "  - **Heap**: Dynamic memory allocation for objects.  \n",
      "  - **Garbage Collection (GC)**: Mark-sweep and mark-compact algorithms to reclaim unused memory.  \n",
      "- **DOM Bridge**: Exposes DOM APIs via a **FFI (Foreign Function Interface)**, incurring latency due to context switches.  \n",
      "\n",
      "---\n",
      "\n",
      "**12.5 GPU Pipeline**  \n",
      "Modern browsers leverage the GPU for rendering:  \n",
      "- **Rasterization**: Converts painted elements into bitmaps (via **Raster Threads**).  \n",
      "- **Compositing**: GPU composites layers into the final frame, enabling 60fps animations.  \n",
      "- **WebGL**: Direct access to GPU via OpenGL/DirectX for 3D graphics.  \n",
      "- **Hardware Acceleration**: Offloads video decoding and complex animations to GPU.  \n",
      "\n",
      "---\n",
      "\n",
      "**12.6 Process Model**  \n",
      "Browsers use a **multi-process architecture** for stability and security:  \n",
      "- **Browser Process**: Manages UI, disk I/O, and main thread scheduling.  \n",
      "- **Renderer Processes**: Isolated per-tab environments executing HTML/JS.  \n",
      "- **GPU Process**: Handles GPU-related tasks.  \n",
      "- **Utility Processes**: Sandboxed workers for plugins or media decoding.  \n",
      "\n",
      "**Isolation**: Each renderer runs in a **sandbox**, restricted by OS-level permissions (e.g., seccomp, namespaces).  \n",
      "\n",
      "---\n",
      "\n",
      "**12.7 Memory Model**  \n",
      "- **Per-Process Memory**: Each renderer has its own heap, preventing memory leaks from affecting others.  \n",
      "- **Memory Pressure Handling**: Triggers GC or discards cached resources when system memory is low.  \n",
      "- **Shared Memory**: IPC uses shared memory regions for large data transfers (e.g., textures).  \n",
      "\n",
      "---\n",
      "\n",
      "**12.8 Scheduling & Event Loop**  \n",
      "The **event loop** manages concurrency on the main thread:  \n",
      "1. **Task Queue**: Macrotasks (e.g., `setTimeout`, I/O) are scheduled in FIFO order.  \n",
      "2. **Microtask Queue**: High-priority tasks (e.g., `Promise`, `MutationObserver`) execute after current task.  \n",
      "3. **Frame Budget**: The browser aims for 16ms per frame (60fps), deferring tasks if necessary.  \n",
      "4. **Workers**: Web Workers run JS in background threads, avoiding blocking the main thread.  \n",
      "\n",
      "**Prioritization**: The scheduler prioritizes user input (e.g., clicks) over rendering or JS.  \n",
      "\n",
      "---\n",
      "\n",
      "**12.9 Inter-Process Communication (IPC)**  \n",
      "Processes communicate via **message-passing** (e.g., **Mojo** in Chrome):  \n",
      "- **Synchronous IPC**: Blocks until a response is received (used sparingly to avoid deadlocks).  \n",
      "- **Asynchronous IPC**: Non-blocking, ideal for rendering updates or network events.  \n",
      "- **Serialization**: Messages are serialized into flat buffers (e.g., **Flattened Message**).  \n",
      "\n",
      "---\n",
      "\n",
      "**12.10 Sandboxing**  \n",
      "- **OS-Level Sandboxing**: Restricts renderer processes from accessing files, network, or hardware.  \n",
      "- **Seccomp-BPF**: Linux kernel filters to block dangerous syscalls.  \n",
      "- **Site Isolation**: Ensures cross-origin data is isolated in separate processes.  \n",
      "- **WebAssembly**: Sandboxed execution of compiled code with memory-safe interfaces.  \n",
      "\n",
      "---\n",
      "\n",
      "**12.11 Conclusion**  \n",
      "A browsers architecture balances performance, security, and compatibility. By isolating processes, optimizing rendering, and leveraging hardware acceleration, browsers deliver seamless web experiences while mitigating security risks. Understanding these systems is critical for optimizing web applications and debugging complex interactions.\n"
     ]
    }
   ],
   "source": [
    "test = groq_smart(\n",
    "    \"Explain the entire inner working of a web browser (network stack, rendering engine, JS engine, GPU pipeline, process model, memory model, scheduling, event loop, IPC, sandboxing). Write it as a full textbook chapter with deep technical detail.\"\n",
    ")\n",
    "print(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6b9c99",
   "metadata": {},
   "source": [
    "11.Build Query Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c0c0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGQueryEngine:\n",
    "\n",
    "    def __init__(self,retriever):\n",
    "        self.retriever = retriever\n",
    "    \n",
    "    def query(self,q):\n",
    "        bundle = QueryBundle(q)\n",
    "        retrieved = self.retriever.retrieve(bundle)\n",
    "\n",
    "        context = \"\\n\\n\".join(n.node.text for n in retrieved)\n",
    "\n",
    "        final_prompt = f\"\"\"\n",
    "\n",
    "        You are a Test Scenario Generation LLM \n",
    "\n",
    "        Use only the context below to answer.\n",
    "\n",
    "\n",
    "        CONTEXT:\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {q}\n",
    "\n",
    "        ANSWER:\n",
    "        \"\"\"\n",
    "        return groq_complete(final_prompt)\n",
    "    \n",
    "rag_engine = RAGQueryEngine(retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a868aa",
   "metadata": {},
   "source": [
    "Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d324e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's tackle this. The user wants test scenarios for user login on IMDb. First, I need to look at the provided context to see what's relevant. The context includes a bunch of URLs and links from IMDb's homepage. There's a \"Sign In\" link here: \"/registration/signin/?ref=nv_generic_lgin&u=\". That's probably where the login functionality is.\n",
      "\n",
      "Now, test scenarios for login usually cover different cases. Let me think. The main ones would be valid credentials, invalid username, invalid password, empty fields, account locked out, third-party login, and maybe password reset. But wait, the context doesn't mention password reset links, so maybe that's not needed here. Also, there's a \"Partially supported\" link related to help, which might be for accessibility or something else. Not sure if that's relevant here.\n",
      "\n",
      "Looking at the Sign In URL, maybe there's a form with username/email and password fields. So test cases should check if those fields are present. Also, what happens when you enter wrong info? The error messages. Also, maybe check if the \"Remember Me\" option works, but the context doesn't mention that. Also, maybe check if after login, the user is redirected correctly. The context has a \"Watchlist\" link that requires being signed in, so maybe after login, accessing that page should work.\n",
      "\n",
      "Wait, the user is asking specifically for test scenarios for user login. So focus on the login process itself. Let me list possible scenarios:\n",
      "\n",
      "1. Valid login with correct username and password.\n",
      "2. Invalid username.\n",
      "3. Valid username, invalid password.\n",
      "4. Empty username field.\n",
      "5. Empty password field.\n",
      "6. Both fields empty.\n",
      "7. Account locked after multiple failed attempts.\n",
      "8. Third-party login (like using Amazon, since IMDb is part of Amazon, but the context doesn't mention third-party options, so maybe skip that).\n",
      "9. Password reset link (but again, not in the context, so maybe not).\n",
      "10. Check if the login form is accessible (maybe related to the \"Partially supported\" link for accessibility).\n",
      "\n",
      "But the context doesn't mention third-party logins or password reset, so maybe stick to the basic ones. Also, check if the login button is present and functional. Also, maybe check for session persistence after login, like if the user stays logged in when closing the browser.\n",
      "\n",
      "Wait, the Sign In URL is \"/registration/signin/?ref=nv_generic_lgin&u=\". So the test scenarios should involve navigating to that URL and testing the login functionality there. Also, after login, the user might be redirected to a specific page, like their watchlist. The context has a \"Watchlist\" link that's under \"/list/watchlist/...\", so maybe after successful login, the user is redirected there. That could be a test case.\n",
      "\n",
      "Also, check for error messages when login fails. The context has a help link for \"Partially supported\" which might relate to accessibility, but maybe not directly relevant here. But maybe include a scenario where the login form is accessible via keyboard navigation or screen readers, but that's more about accessibility testing, which might be beyond basic login scenarios unless specified.\n",
      "\n",
      "So, compiling all that, the test scenarios would be:\n",
      "\n",
      "1. Valid login with correct credentials.\n",
      "2. Invalid username.\n",
      "3. Valid username, invalid password.\n",
      "4. Empty username.\n",
      "5. Empty password.\n",
      "6. Both fields empty.\n",
      "7. Account locked after multiple failed attempts.\n",
      "8. Successful login redirects to the intended page (e.g., Watchlist).\n",
      "9. Check for presence of login form elements (username, password, submit button).\n",
      "10. Check for error messages when invalid credentials are entered.\n",
      "\n",
      "But need to make sure these are based on the context provided. The context doesn't mention account lockout, but it's a common scenario. Maybe include it. Also, the \"Sign In\" URL is part of the registration path, so maybe after login, the user is redirected to their profile or another page. The Watchlist link is under \"/list/watchlist/...\", so maybe after login, the user is taken there. That's a possible test case.\n",
      "\n",
      "Also, check if the login form is accessible. The context mentions \"Partially supported\" in a couple of places, which might relate to accessibility features. So maybe a test scenario for accessibility compliance, like keyboard navigation or screen reader compatibility. But the user might not need that unless specified. However, since the context includes that, maybe include a basic accessibility check.\n",
      "\n",
      "So, final list:\n",
      "\n",
      "1. Valid login with correct credentials.\n",
      "2. Invalid username.\n",
      "3. Valid username, invalid password.\n",
      "4. Empty username field.\n",
      "5. Empty password field.\n",
      "6. Both fields empty.\n",
      "7. Account locked after multiple failed attempts.\n",
      "8. Successful login redirects to Watchlist page.\n",
      "9. Presence and functionality of login form elements.\n",
      "10. Error messages displayed for invalid attempts.\n",
      "11. Accessibility check for login form (keyboard navigation, screen reader).\n",
      "\n",
      "But maybe the user wants only the ones directly related to the context. Since the context includes the Sign In URL and the Watchlist, maybe focus on those. Also, the \"Help\" link might be for support, but not directly related to login scenarios. So perhaps stick to the first 8 scenarios, ensuring they align with the provided context.\n",
      "</think>\n",
      "\n",
      "**Test Scenarios for User Login on IMDb**  \n",
      "\n",
      "1. **Valid Login with Correct Credentials**  \n",
      "   - **Description**: Verify that a user can successfully log in with a valid username/email and password.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the IMDb Sign In page (`/registration/signin/?ref=nv_generic_lgin&u=`).  \n",
      "     2. Enter a valid registered email/username and password.  \n",
      "     3. Click the \"Sign In\" button.  \n",
      "   - **Expected Result**: User is redirected to their profile/dashboard (e.g., Watchlist page `/list/watchlist/...`).  \n",
      "\n",
      "2. **Invalid Username**  \n",
      "   - **Description**: Test login with an unregistered username/email.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the Sign In page.  \n",
      "     2. Enter an invalid username/email and a valid password.  \n",
      "     3. Click \"Sign In\".  \n",
      "   - **Expected Result**: Error message displayed (e.g., \"Invalid email or password\").  \n",
      "\n",
      "3. **Valid Username, Invalid Password**  \n",
      "   - **Description**: Test login with a valid username/email and incorrect password.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the Sign In page.  \n",
      "     2. Enter a valid username/email and an incorrect password.  \n",
      "     3. Click \"Sign In\".  \n",
      "   - **Expected Result**: Error message displayed (e.g., \"Invalid email or password\").  \n",
      "\n",
      "4. **Empty Username Field**  \n",
      "   - **Description**: Test login with an empty username/email field.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the Sign In page.  \n",
      "     2. Leave the username/email field blank and enter a password.  \n",
      "     3. Click \"Sign In\".  \n",
      "   - **Expected Result**: Error message for missing username/email.  \n",
      "\n",
      "5. **Empty Password Field**  \n",
      "   - **Description**: Test login with an empty password field.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the Sign In page.  \n",
      "     2. Enter a valid username/email and leave the password field blank.  \n",
      "     3. Click \"Sign In\".  \n",
      "   - **Expected Result**: Error message for missing password.  \n",
      "\n",
      "6. **Both Fields Empty**  \n",
      "   - **Description**: Test login with both username/email and password fields empty.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the Sign In page.  \n",
      "     2. Leave both fields blank.  \n",
      "     3. Click \"Sign In\".  \n",
      "   - **Expected Result**: Error messages for missing username/email and password.  \n",
      "\n",
      "7. **Account Locked After Multiple Failed Attempts**  \n",
      "   - **Description**: Verify account lockout after repeated failed login attempts.  \n",
      "   - **Steps**:  \n",
      "     1. Attempt to log in with incorrect credentials 5+ times.  \n",
      "     2. Try to log in again.  \n",
      "   - **Expected Result**: Account is temporarily locked, and a message is displayed (e.g., \"Too many attempts. Try again later\").  \n",
      "\n",
      "8. **Post-Login Redirection to Watchlist**  \n",
      "   - **Description**: Verify redirection to the Watchlist page after successful login.  \n",
      "   - **Steps**:  \n",
      "     1. Log in with valid credentials.  \n",
      "     2. Check the URL and page content.  \n",
      "   - **Expected Result**: User is redirected to `/list/watchlist/...` and sees their watchlist.  \n",
      "\n",
      "9. **Presence of Login Form Elements**  \n",
      "   - **Description**: Ensure the login form includes required fields and buttons.  \n",
      "   - **Steps**:  \n",
      "     1. Navigate to the Sign In page.  \n",
      "     2. Verify the presence of username/email field, password field, and \"Sign In\" button.  \n",
      "   - **Expected Result**: All required elements are visible and functional.  \n",
      "\n",
      "10. **Accessibility Check for Login Form**  \n",
      "    - **Description**: Validate login form accessibility (keyboard navigation, screen reader compatibility).  \n",
      "    - **Steps**:  \n",
      "      1. Use keyboard navigation to tab through login fields.  \n",
      "      2. Use a screen reader to verify field labels and error messages.  \n",
      "    - **Expected Result**: Form is navigable via keyboard and compatible with screen readers (aligned with \"Partially supported\" accessibility note in context).  \n",
      "\n",
      "**Note**: Scenarios reference URLs and features explicitly mentioned in the provided context (e.g., `/registration/signin/`, `/list/watchlist/`).\n"
     ]
    }
   ],
   "source": [
    "print(rag_engine.query(\"Generate test scenarios for user login.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78da53a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
